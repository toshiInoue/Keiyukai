{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toshiInoue/Keiyukai/blob/main/adaboost%2Bqwen%2Bbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "VDS_dS0uGwH_",
        "outputId": "9a815533-42c6-4cef-d17c-44b1ae918ee7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.6\n",
            "Collecting git+https://github.com/huggingface/accelerate\n",
            "  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-g6bsvlw5\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-g6bsvlw5\n",
            "  Resolved https://github.com/huggingface/accelerate to commit 16b6b3f73f01cd8e4abe0e65720894162b0bb277\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.13.0.dev0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.13.0.dev0) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.13.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate==1.13.0.dev0) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.13.0.dev0) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.13.0.dev0) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.13.0.dev0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.13.0.dev0) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.13.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.13.0.dev0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate==1.13.0.dev0) (2025.11.12)\n",
            "Building wheels for collected packages: accelerate\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for accelerate: filename=accelerate-1.13.0.dev0-py3-none-any.whl size=381523 sha256=df3292e02db486767e12ecd5e614e8a660b55e473a7b6e6d5c672a4d0280bd2c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v_hj1v2d/wheels/5a/6d/23/a920341d792c2e4a6f96daf5449e133329e4444224b0ea716b\n",
            "Successfully built accelerate\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "Successfully installed accelerate-1.13.0.dev0\n",
            "Collecting fugashi\n",
            "  Downloading fugashi-1.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.3 kB)\n",
            "Downloading fugashi-1.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (694 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m694.9/694.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fugashi\n",
            "Successfully installed fugashi-1.5.2\n",
            "Collecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556704 sha256=e6817504d686679d96d6a3d51200336890081bd8b7efc015f77e36268c62ada2\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/8b/55/dd5978a069678c372520847cf84ba2ec539cb41917c00a2206\n",
            "Successfully built ipadic\n",
            "Installing collected packages: ipadic\n",
            "Successfully installed ipadic-1.0.0\n",
            "Collecting unidic-lite\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658817 sha256=fee586d463c0758b606c2d02c47b54636b4d9d5833e700a4d2930b22d24bc562\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/1f/0f/4d43887e5476d956fae828ee9b6687becd5544d68b51ed633d\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install git+https://github.com/huggingface/accelerate\n",
        "!pip install fugashi\n",
        "!pip install ipadic\n",
        "!pip install unidic-lite\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_curve, auc, RocCurveDisplay\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qROVaaS9HMV1",
        "outputId": "ee9519ee-e35f-4230-da2b-e1fef8aadfcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/models/adaboost_model.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1860358105.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/models/adaboost_model.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             with _validate_fileobject_and_memmap(f, filename, mmap_mode) as (\n\u001b[1;32m    737\u001b[0m                 \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/models/adaboost_model.pkl'"
          ]
        }
      ],
      "source": [
        " model = joblib.load(\"/content/drive/MyDrive/Colab Notebooks/models/adaboost_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LrOG-a0IRq9"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/machine_learning_TOS.csv', encoding='ms932', sep=',', skiprows=0)\n",
        "t = df['Diagnosis'].values\n",
        "features = df.iloc[:,5:]\n",
        "cvt1 = {'非常に困難':2,'やや困難':1,'問題なし':0,'しない':0}\n",
        "cvt2 = {'＋＋':2,'＋':1,'－':0}\n",
        "\n",
        "feature=[]\n",
        "for i,f in features.iterrows():\n",
        "    #シャンプー\tドライヤー\tつり革\t携帯電話\t書字\t鎖骨上窩_圧痛\t斜角筋三角_圧痛\t小胸筋_圧痛\tRoos\tWright\tMorley\n",
        "\n",
        "    num_vector =[cvt1[f['シャンプー']],cvt1[f['ドライヤー']],cvt1[f['つり革']],cvt1[f['携帯電話']],cvt1[f['書字']],\n",
        "    cvt2[f['鎖骨上窩_圧痛']],cvt2[f['斜角筋三角_圧痛']],cvt2[f['小胸筋_圧痛']],cvt2[f['Roos']],cvt2[f['Wright']],cvt2[f['Morley']]]\n",
        "    feature.append(num_vector)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(feature, t, stratify=t, random_state=42)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(f\"オーバーサンプリング前のクラス分布: {Counter(y_train)}\")\n",
        "print(f\"オーバーサンプリング前の X_train の形状: {X_train.shape}\")\n",
        "\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "X_test, y_test = rus.fit_resample(X_test, y_test)\n",
        "\n",
        "smote = BorderlineSMOTE(random_state=42)\n",
        "\n",
        "# 2. fit_resample を使ってオーバーサンプリングを実行\n",
        "#    X_train と Y_train を渡し、新しい X と Y を受け取ります\n",
        "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "# --- 結果の確認 ---\n",
        "print(\"\\n--- オーバーサンプリング後 ---\")\n",
        "print(f\"オーバーサンプリング後のクラス分布: {Counter(y_train)}\")\n",
        "print(f\"オーバーサンプリング後の X_train の形状: {X_train.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWd-JppFKMrm"
      },
      "outputs": [],
      "source": [
        "proba_all_train = model.predict_proba(X_train[:, :5])\n",
        "proba_train = proba_all_train[:, 1]\n",
        "proba_all_test = model.predict_proba(np.array(X_test)[:, :5])\n",
        "proba_test = proba_all_test[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7M4B8jNQ4eQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "import time # 処理時間の計測用\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1. LLMパイプラインの初期化（関数外で一度だけ行う）\n",
        "# -------------------------------------------------------------------------\n",
        "def initialize_pipeline(model_name=\"Qwen/Qwen2.5-3B-Instruct\"):\n",
        "    \"\"\"\n",
        "    モデルを指定して、Hugging Faceのpipelineを初期化します。\n",
        "    \"\"\"\n",
        "    #print(f\"--- {model_name} の読み込みを開始します... ---\")\n",
        "    try:\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_name,\n",
        "            torch_dtype=\"auto\", # FP16/BF16を自動選択\n",
        "            device_map=\"auto\" # GPUを自動割り当て\n",
        "        )\n",
        "        #print(\"--- パイプラインの準備が完了しました。---\")\n",
        "        return pipe\n",
        "    except Exception as e:\n",
        "        #print(f\"--- パイプラインの初期化に失敗: {e} ---\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbT-jHIASgkU"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# 2. プロンプトを生成する関数\n",
        "# -------------------------------------------------------------------------\n",
        "def create_chat_messages(data_list,proba):\n",
        "    \"\"\"\n",
        "    診察データ（リスト）を受け取り、LLMに渡すための\n",
        "    「チャット履歴（messages）のリスト」を1件分作成して返します。\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 2-1. プロンプトのテンプレートを定義 ---\n",
        "\n",
        "    # システムプロンプト (役割、指示、ルール)\n",
        "    system_prompt = \"\"\"\n",
        "# 役割\n",
        "あなたは医師のアシスタントです。患者の診察データを、診察用の自然なサマリー文章に変換してください。\n",
        "\n",
        "# 指示\n",
        "以下の「入力データ」を「ルールの定義」に従って、自然な日本語の文章に要約してください。「例文」を参考にして、同じような形式で出力してください。\n",
        "\n",
        "# ルールの定義\n",
        "* 0: 結果は「陰性 (-/Minus)」\n",
        "* 1: 結果は「陽性 (+/Plus)」\n",
        "* 2: 結果は「強陽性 (++/Double Plus)」\n",
        "\n",
        "# 陽性確率: 事前の診察から予測された陽性確率です\n",
        "\"\"\"\n",
        "\n",
        "    # メッセージの骨組み（例文を含む）\n",
        "    #鎖骨上窩_圧痛\t斜角筋三角_圧痛\t小胸筋_圧痛\tRoos\tWright\tMorley\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "\n",
        "        # --- 例文1 ---※ここを変えてください\n",
        "        {\"role\": \"user\", \"content\": \"\"\"### 入力データ\n",
        "        * 鎖骨上窩_圧痛: 0\n",
        "        * 斜角筋三角_圧痛: 1\n",
        "        * 小胸筋_圧痛: 0\n",
        "        * Roos: 0\n",
        "        * Wright: 1\n",
        "        * Morley: 0\n",
        "        * 陽性確率: 0.56\"\"\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"所見サマリー：\\n圧痛に関しては、斜角筋三角に陽性(+)所見を認めますが、鎖骨上窩および小胸筋は陰性(-)です。\\n誘発テストではWrightテストのみ陽性(+)であり、RoosおよびMorleyテストは陰性(-)でした。\\n事前の診察による陽性率は0.56でした。\"},\n",
        "\n",
        "        # --- 例文2 ---※ここを変えてください\n",
        "        {\"role\": \"user\", \"content\": \"\"\"### 入力データ\n",
        "        * 鎖骨上窩_圧痛: 2\n",
        "        * 斜角筋三角_圧痛: 2\n",
        "        * 小胸筋_圧痛: 2\n",
        "        * Roos: 2\n",
        "        * Wright: 2\n",
        "        * Morley: 1\n",
        "        * 陽性確率: 0.82\"\"\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"所見サマリー：\\n鎖骨上窩、斜角筋三角、小胸筋の全ての部位において、著明な圧痛(++)を認めます。\\n誘発テストにおいてもRoosおよびWrightテストで強陽性(++)を示し、Morleyテストも陽性(+)です。全体的に症状が強く誘発されています。\\n事前の診察による陽性率は0.82でした。\"},\n",
        "    ]\n",
        "\n",
        "    # --- 2-2. 受け取ったデータから本番用の入力文字列を生成 ---\n",
        "    # data_dict = {\"シャンプー\": 2, \"ドライヤー\": 3, ...}\n",
        "    # を\n",
        "    # \"* シャンプー: 2\\n* ドライヤー: 3\\n...\" という文字列に変換\n",
        "\n",
        "    item = [\"鎖骨上窩_圧痛\",\"斜角筋三角_圧痛\",\"小胸筋_圧痛\",\"Roos\",\"Wright\",\"Morley\"]\n",
        "\n",
        "    data_lines = []\n",
        "    for i, value in enumerate(data_list):\n",
        "        data_lines.append(f\"* {item[i]}: {value}\")\n",
        "\n",
        "    # \\n (改行) で結合\n",
        "    user_input_text = \"### 入力データ\\n\" + \"\\n\".join(data_lines)+f\"* 陽性確立: {proba:.2f}\"\n",
        "\n",
        "    # --- 2-3. メッセージリストに本番データを追加 ---\n",
        "    messages.append({\"role\": \"user\", \"content\": user_input_text})\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prompt(llm_pipe, all_patient_data,proba_list):\n",
        "  all_messages_list = [create_chat_messages(data,proba) for data,proba in zip(all_patient_data[:,5:],proba_list)]\n",
        "  try:\n",
        "    results = llm_pipe(\n",
        "        all_messages_list,\n",
        "        batch_size=20,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        eos_token_id=llm_pipe.tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"--- バッチ処理中にエラーが発生しました: {e} ---\")\n",
        "  output = []\n",
        "  for result in results:\n",
        "    output.append(result[0][\"generated_text\"][-1][\"content\"].strip())\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "ifbO7FpjpKto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k53pUap-RhjH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#かなり時間かかるので注意\n",
        "llm_pipe = initialize_pipeline()\n",
        "X_test=generate_prompt(llm_pipe,np.array(X_test),proba_test)\n",
        "texts=generate_prompt(llm_pipe,X_train,proba_train)\n",
        "t=y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyxC2OgBaxzz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# --- 2. データセットの準備 ---\n",
        "\n",
        "# (1) データを「訓練用」と「検証用」に分割\n",
        "# （データが少ないため、記事の「テスト用」は省略し、「検証用」で代用します）\n",
        "# （データが多い場合は、train_test_splitを2回使い、訓練/検証/テストに分けてください）\n",
        "SEED = 42\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts, t, test_size=0.2, random_state=SEED, stratify=t\n",
        ")\n",
        "\n",
        "# (2) Hugging FaceのDataset形式に変換\n",
        "# Pythonの辞書(dict)としてデータを作成\n",
        "train_data = {'text': train_texts, 'label': train_labels}\n",
        "val_data = {'text': val_texts, 'label': val_labels}\n",
        "\n",
        "# Datasetオブジェクトを作成\n",
        "train_dataset_raw = Dataset.from_dict(train_data)\n",
        "val_dataset_raw = Dataset.from_dict(val_data)\n",
        "\n",
        "# (3) 訓練・検証データセットをまとめる\n",
        "dataset = DatasetDict({\n",
        "    'train': train_dataset_raw,\n",
        "    'validation': val_dataset_raw\n",
        "})\n",
        "\n",
        "print(\"データセットの準備完了:\")\n",
        "print(dataset)\n",
        "\n",
        "# --- 3. トークナイザの準備 ---\n",
        "model_ckpt = \"cl-tohoku/bert-base-japanese\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "# --- 4. トークン化関数の定義と実行 ---\n",
        "def tokenize(batch):\n",
        "    # padding=True: バッチ内の最大長に合わせてパディング\n",
        "    # truncation=True: モデルの最大長を超える場合はカット\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "# .map() を使ってデータセット全体をトークン化\n",
        "# （記事と異なり、labelは既に含まれているのでtokenizerの結果をそのまま返す）\n",
        "dataset_encoded = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Trainerで使うために、不要な'text'カラムを削除\n",
        "dataset_encoded = dataset_encoded.remove_columns([\"text\"])\n",
        "\n",
        "# PyTorchテンソル形式に設定\n",
        "dataset_encoded.set_format(\"torch\")\n",
        "\n",
        "print(\"\\nトークン化後のデータセット:\")\n",
        "print(dataset_encoded)\n",
        "\n",
        "# 使いやすいように変数に代入\n",
        "small_train_dataset = dataset_encoded['train']\n",
        "small_valid_dataset = dataset_encoded['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aWahqnFY9C1"
      },
      "outputs": [],
      "source": [
        "# --- 5. モデルの読み込み ---\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# ★ クラス数を 2 に設定\n",
        "num_labels = 2\n",
        "batch_size = 4 # データが少ないのでバッチサイズを小さめに設定\n",
        "model_name = \"my-text-classification-bert\" # モデル名（任意）\n",
        "\n",
        "model = (AutoModelForSequenceClassification\n",
        "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
        "         .to(device))\n",
        "\n",
        "# --- 6. 評価関数の定義 ---\n",
        "# (記事と全く同じ)\n",
        "def compute_metrics(pred):\n",
        "    preds, labels = pred\n",
        "    preds = preds.argmax(-1)\n",
        "\n",
        "    # F1スコアのaverageを 'binary' に変更（2クラス分類の場合）\n",
        "    f1 = f1_score(labels, preds, average=\"binary\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dv3zXqQVZDxH"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# --- 7. 学習パラメータの設定 ---\n",
        "\n",
        "# 1エポックあたりのステップ数を計算\n",
        "# (訓練データ数 / バッチサイズ) の切り上げ\n",
        "steps_per_epoch = (len(small_train_dataset) + batch_size - 1) // batch_size\n",
        "\n",
        "batch_size = 4\n",
        "model_name = \"my-text-classification-bert\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_name,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    eval_steps=steps_per_epoch,     # 1エポック終了ごとに評価\n",
        "    logging_steps=steps_per_epoch,  # 1エポック終了ごとにログ記録\n",
        "    disable_tqdm=False,\n",
        "    log_level=\"error\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# --- 8. Trainerの初期化と学習開始 ---\n",
        "# （ここは変更なし）\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_valid_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(\"\\n--- 学習を開始します ---\")\n",
        "trainer.train()\n",
        "print(\"--- 学習が完了しました ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxPcOZzdbXdP"
      },
      "outputs": [],
      "source": [
        "# (1) pipelineを作成\n",
        "# 'pipeline'が未定義というエラーを解消\n",
        "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=model.device.index)\n",
        "\n",
        "# --- 1. 'pipe' を使って X_test のテキストを予測 ---\n",
        "# (pipe は前のステップで作成・import済みと仮定)\n",
        "# 'pipe' が 'pipeline' is not defined エラーになる場合は、\n",
        "# from transformers import pipeline\n",
        "# をこのセルの先頭で実行してください。\n",
        "\n",
        "print(\"推論を実行中...\")\n",
        "pred_results = pipe(X_test)\n",
        "print(\"推論完了。\")\n",
        "\n",
        "# --- 2. 予測結果を 0 と 1 のリストに変換 ---\n",
        "# pipeの出力は {'label': 'LABEL_1', 'score': 0.99} のような形式です。\n",
        "# 'LABEL_1' を 1 に、'LABEL_0' を 0 に変換します。\n",
        "\n",
        "def convert_label_to_int(label_str):\n",
        "    \"\"\"'LABEL_1' を 1 に、'LABEL_0' を 0 に変換\"\"\"\n",
        "    return 1 if label_str == 'LABEL_1' else 0\n",
        "\n",
        "y_preds = [convert_label_to_int(result['label']) for result in pred_results]\n",
        "\n",
        "# --- 3. 混同行列の計算と描画 ---\n",
        "# 正解ラベルと予測ラベルを比較\n",
        "print(f\"正解ラベル (y_test): {y_test}\")\n",
        "print(f\"予測ラベル (y_preds): {y_preds}\")\n",
        "\n",
        "# 混同行列を計算\n",
        "# normalize=\"true\" にすると、割合(%)で表示されます\n",
        "cm = confusion_matrix(y_test, y_preds)\n",
        "cm_custom = np.array([[cm[1,1], cm[1,0]],\n",
        "                      [cm[0,1], cm[0,0]]])\n",
        "labels = ['Positive', 'Negative']\n",
        "\n",
        "# 混同行列を描画\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm_custom,\n",
        "    display_labels=labels\n",
        ")\n",
        "\n",
        "# グラフのカスタマイズ (記事のコードに準拠)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCLx68BPba_5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. クラスの表示名を定義します (混同行列で使ったものと同じ)\n",
        "# (0 = Negative, 1 = Positive)\n",
        "target_names = [\"Negative (0)\", \"Positive (1)\"]\n",
        "\n",
        "# 2. レポートを生成\n",
        "# 必要なのは y_test (正解) と y_preds (予測) だけです\n",
        "report = classification_report(\n",
        "    y_test,\n",
        "    y_preds,\n",
        "    target_names=target_names\n",
        ")\n",
        "\n",
        "# 3. レポートを出力\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll8tnPrTWdzg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6zKzmKLyDGYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BiH3DJkGEK5C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}